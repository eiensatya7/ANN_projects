{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "pycharm-53d5b8fb",
      "language": "python",
      "display_name": "PyCharm (pythonProject)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "miniproject_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eiensatya7/ANN_projects/blob/main/miniproject_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Pm_mbIZvX9G"
      },
      "source": [
        "# Problem :\n",
        "\n",
        "IMDB movie review sentiment classification problem. Each movie review is a variable sequence of words and the sentiment of each movie review must be classified. The IMDB Movie Review Dataset contains 25,000 highly-polar movie reviews (good or bad) for training and the same amount again for testing. The problem is to determine whether a given movie review has a positive or negative sentiment. Keras provides access to the IMDB dataset built-in. The imdb.load_data() function allows you to load the dataset in a format that is ready for use in neural network and deep learning models. The words have been replaced by integers that indicate the ordered frequency of each word in the dataset. The sentences in each review are therefore comprised of a sequence of integers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCqbxtfvvX9H"
      },
      "source": [
        "# Why CNN with LSTM for text Classifcation\n",
        "\n",
        "CNNs are generally used in computer vision, however they’ve recently been applied to various NLP tasks and the results were promising.\n",
        "Let’s briefly see what happens when we use CNN on text data through a diagram.The result of each convolution will fire when a special pattern is detected. By varying the size of the kernels and concatenating their outputs, you’re allowing yourself to detect patterns of multiples sizes (2, 3, or 5 adjacent words).Patterns could be expressions (word ngrams?) like “I hate”, “very good” and therefore CNNs can identify them in the sentence regardless of their position.\n",
        "Recurrent neural networks can obtain context information but the order of words will lead to bias; the text analysis method based on Convolutional neural network (CNN) can obtain important features of text through pooling but it is difficult to obtain contextual information which can be leverage using LSTM. So using the combination of CNN with LSTM could give us some intresting results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd4wQ7CZvX9H"
      },
      "source": [
        "# Develop an text classification model based on CNN + LSTM in Keras.\n",
        "\n",
        "In this assignment, you will have to train two Text classification:\n",
        "1) LSTM based Text Classification\n",
        "2) CNN + LSTM based Text Classification\n",
        "\n",
        "After training the two different classification, you have to compare the F1 Score on both of the model trained and report the best F1 Score for which of them.\n",
        "\n",
        "This notebook is divided into six parts. Total : [12 Marks]\n",
        "\n",
        "1. Calculate the average length of reviews and modifying the length of sentences in X_train , X_test, X_cv [2 Mark]\n",
        "2. Implement the LSTM model [3 Marks]\n",
        "3. Calculate the LSTM model F1 Score [1 Mark]\n",
        "4. Implement the CNN + LSTM [3 Marks]\n",
        "5. Calculate the CNN + LSTM model F1 Score [1 Mark]\n",
        "6. Identify and decode 5 of the sentences misclassified by LSTM model which  were correctly classified by CNN + LSTM model [2 Mark]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsrle0cvvX9I"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import imdb\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences  # to do padding or truncating\n",
        "from tensorflow.keras.models import Sequential  # the model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense  # layers of the architecture\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "#import the required library\n",
        "\n",
        "# Student will have to code here\n",
        "\n",
        "\n",
        "# Students will end their code here"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUnfSbvuvX9J",
        "outputId": "56244c5a-53e0-444c-abd3-44bc2a26d6a5"
      },
      "source": [
        "# load the dataset but only keep the top n words, zero the rest\n",
        "top_words = 10000\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "np.load.__defaults__ = (None, True, True, 'ASCII')\n",
        "\n",
        "# call load_data with allow_pickle implicitly set to true\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)\n",
        "\n",
        "X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.2)\n",
        "print(\"Shape of train data:\", X_train.shape)\n",
        "print(\"Shape of Test data:\", X_test.shape)\n",
        "print(\"Shape of CV data:\", X_cv.shape)\n",
        "\n",
        "print(X_train)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of train data: (20000,)\n",
            "Shape of Test data: (25000,)\n",
            "Shape of CV data: (5000,)\n",
            "[list([1, 260, 332, 2, 274, 13, 16, 1076, 685, 11, 4, 201, 21, 32, 11, 32, 12, 9, 179, 7180, 886, 4, 85, 795, 12, 9, 2, 8, 124, 15, 4, 2, 2, 26, 254, 33, 157, 319, 2, 23, 2, 2, 467, 175, 2, 10, 10, 9, 3643, 6, 117, 5648, 252, 40, 6, 176, 7, 2, 29, 1085, 27, 2604, 17, 4, 91, 674, 155, 126, 29, 1077, 4, 5263, 2, 2, 5, 2, 33, 1573, 175, 1434, 315, 4, 201, 75, 79, 12, 103, 44, 4, 86, 158, 234, 10, 10, 9, 3643, 6, 117, 4274, 11, 4, 201, 252, 4, 173, 44, 4, 2, 11, 1226, 938, 9, 572, 1139, 2, 49, 55, 196, 2732, 479, 180, 8, 2, 9092, 5, 2, 2, 56, 4, 226, 9182, 7, 1226, 938, 11, 44, 1119, 234, 21, 4, 213, 1287, 7513, 134, 183, 122, 11, 192, 6558, 21, 24, 484, 6771, 4, 1007, 18, 4, 2, 1023, 429, 4, 2921, 6714, 10, 10, 9, 29, 7656, 429, 1010, 4707, 11, 101, 96, 5898, 24, 6, 681, 24, 8, 61, 4889, 32, 29, 560, 9, 15, 1996, 256, 6, 1058, 173, 11, 2, 63, 6714, 4677, 53, 946, 24, 15, 1996, 9, 4, 64, 282, 10, 10, 11, 4, 130, 48, 335, 267, 18, 142, 15, 2, 129, 205, 281, 7, 2, 95, 14, 201, 9, 24, 18, 25, 21, 48, 25, 26, 928, 11, 32, 7, 4, 6229, 74, 2398, 89, 9609, 3115, 42, 1854, 14, 201, 2423, 6, 4507, 2956, 7, 4, 1379, 2314])\n",
            " list([1, 24, 196, 195, 8, 30, 792, 1615, 5, 24, 5469, 195, 8, 6, 346, 14, 155, 2981, 18, 31, 282, 8, 28, 6, 2493, 289, 96, 50, 26, 433, 1007, 8, 1778, 31, 619, 155, 9, 15, 14, 100, 28, 93, 6, 542, 792, 1615, 20, 7594, 2352, 2, 83, 41, 4267, 5, 9, 6, 55, 9426, 250, 5, 54, 84, 26, 38, 2, 19, 6, 500, 109, 40, 7978, 2, 15, 36, 97, 2516, 2, 7, 41, 25, 124, 15, 6, 1796, 2026, 509, 9, 270, 8, 7751, 472, 14, 9, 184, 1235, 262, 4, 2149, 1214, 383, 136, 200, 2, 5, 7594, 121, 25, 70, 67, 41, 508, 9, 2, 945, 106, 14, 48, 25, 43, 191, 79, 195, 7, 7594, 42, 4647, 37, 166, 6, 327, 1976, 5, 47, 2, 2, 2, 2, 126])\n",
            " list([1, 159, 4, 766, 7, 742, 9046, 512, 7827, 314, 7, 4, 581, 351, 1145, 71, 2375, 73, 2, 2373, 36, 434, 69, 76, 128, 2702, 7131, 11, 4, 154, 504, 21, 1031, 2, 1212, 51, 3487, 122, 134, 402, 1145, 1464, 8, 4, 20, 170, 1071, 4235, 34, 14, 22, 600, 1689, 10, 10, 4, 65, 9, 44, 35, 5144, 8, 9883, 625, 1288, 9, 8, 169, 5, 2330, 4, 1003, 7, 2, 31, 7, 4, 1073, 2147, 4, 3908, 23, 27, 205, 5, 732, 44, 1430, 27, 865, 1272, 10, 10, 14, 22, 9, 691, 6, 119, 5929, 19, 1145, 21, 319, 17, 14, 9, 6, 5634, 20, 4, 301, 1145, 26, 53, 40, 2, 74, 4, 2120, 1889, 2593, 75, 104, 7, 639, 36, 306, 8, 8261, 8, 330, 1140, 247, 74, 2, 2, 5, 179, 2035, 4, 2, 9, 643, 2, 99, 4, 226, 155, 9, 66, 55, 753, 1212, 39, 4, 583, 7, 189, 50, 218, 101, 120, 4, 350, 3650, 2, 8, 401, 178, 2176, 12, 186, 2388, 15, 14, 2856, 2426, 76, 722, 60, 4609, 153, 596, 67, 12, 48, 25, 28, 8, 67, 285, 19, 2, 11, 4, 425, 21, 900, 13, 62, 4666, 9422, 14, 31])\n",
            " ...\n",
            " list([1, 2009, 1386, 14, 22, 9, 1380, 12, 47, 49, 1917, 328, 1717, 388, 21, 23, 4, 226, 4, 114, 9, 55, 2711, 17, 9, 94, 5981, 749, 48, 335, 6, 337, 7, 120, 4, 350, 567, 11, 2483, 102, 40, 7281, 42, 219, 490, 119, 12, 48, 335, 267, 18, 142, 33, 32, 312, 7105, 7010, 245, 13, 219, 12, 17, 173, 7, 4, 2, 22, 1413, 2, 5, 13, 64, 2471, 12, 88, 13, 16, 267, 18, 142, 1193, 1116, 12, 218, 1193, 43, 2, 5, 4009, 8, 870, 143, 48, 25, 2070, 181, 8, 30, 5235, 140, 67, 142, 40, 4, 416, 48, 25, 181, 8, 30, 2, 46, 42, 376, 129, 369, 44, 6, 66, 4621, 56, 22, 95, 14, 9, 18, 25])\n",
            " list([1, 50, 9, 38, 76, 7, 290, 11, 14, 20, 15, 12, 9, 254, 8, 124, 121, 8, 895, 19, 2822, 387, 72, 895, 34, 9255, 61, 8170, 18, 6, 404, 1146, 34, 8298, 2, 15, 41, 239, 1407, 46, 11, 4, 321, 177, 9, 2822, 849, 670, 2, 47, 5077, 685, 148, 7, 178, 37, 28, 6744, 27, 157, 175, 2122, 7, 4, 132, 11, 4, 1884, 9, 2314, 7, 22, 231, 33, 94, 118])\n",
            " list([1, 54, 13, 86, 219, 14, 22, 11, 1316, 11, 7553, 13, 16, 2, 120, 13, 69, 115, 110, 6, 22, 40, 14, 159, 12, 69, 6, 680, 8772, 962, 179, 1025, 4, 108, 15, 13, 69, 110, 2453, 5, 12, 317, 6, 5863, 1488, 10, 10, 13, 264, 15, 2, 2, 2, 2, 26, 2481, 48, 24, 5270, 434, 4, 4347, 9, 480, 11, 94, 671, 8, 2398, 4, 529, 10, 10, 48, 14, 47, 77, 9450, 8, 288, 11, 5425, 241, 2718, 5, 11, 4, 204, 785, 1101, 13, 62, 119, 8, 842, 44, 12, 121, 12, 70, 30, 4662])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXI9dZl3vX9J",
        "outputId": "71f77ba8-7a71-43c2-c734-2da84cecf783"
      },
      "source": [
        "def get_avg_length(data):\n",
        "    review_length = []\n",
        "    largest_length = 0\n",
        "    smallest_length = 1000000\n",
        "    for review in data:\n",
        "        length = len(review)\n",
        "        largest_length = length if largest_length < length else largest_length\n",
        "        smallest_length = length if smallest_length > length else smallest_length\n",
        "        review_length.append(length)\n",
        "    average_review_length = int(np.ceil(np.mean(review_length)))\n",
        "    return average_review_length, largest_length, smallest_length\n",
        "\n",
        "\n",
        "# truncate and pad input sequences\n",
        "# Calculate the average length of reviews using the training set (X_train) and set the value to max_review_length\n",
        "# truncate or pad the reviews so that length of all the reviews are same\n",
        "\n",
        "average_review_length, largest_length, smallest_length = get_avg_length(X_train)\n",
        "\n",
        "print(\n",
        "    f\"average review length {average_review_length}, smallest review length {smallest_length}, largest review length {largest_length}\")\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen=average_review_length, padding='post',\n",
        "                        truncating='post')  # code to truncate or pad the reviews to the average_review_length\n",
        "X_test = pad_sequences(X_test, maxlen=average_review_length, padding='post',\n",
        "                       truncating='post')  # code to truncate or pad the reviews to the average_review_length\n",
        "X_cv = pad_sequences(X_cv, maxlen=average_review_length, padding='post',\n",
        "                     truncating='post')\n",
        "\n",
        "print(\"Padded and truncated posts\")\n",
        "\n",
        "average_review_length, largest_length, smallest_length = get_avg_length(X_train)\n",
        "print(\n",
        "    f\"average review length {average_review_length}, smallest review length {smallest_length}, largest review length {largest_length}\")\n",
        "\n",
        "# code to truncate or pad the reviews to the average_review_length"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average review length 240, smallest review length 11, largest review length 2494\n",
            "Padded and truncated posts\n",
            "average review length 240, smallest review length 240, largest review length 240\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LFZ5a40vX9K",
        "outputId": "a4b40c10-d6d6-4306-f37b-3be4b2028043"
      },
      "source": [
        "# Decoding the data coded data of IMDB ( Data Understanding )\n",
        "index = imdb.get_word_index()\n",
        "reverse_index = dict([(value, key) for (key, value) in index.items()])\n",
        "decoded = \" \".join([reverse_index.get(i - 3, \"#\") for i in X_train[0]])\n",
        "print(decoded)\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# having read # book i was slightly disappointed in the series but all in all it is quite informative reading the other comments it is # to know that the # # are hard at work seeing # on # # under every # br br is diamond a little preachy sure like a lot of # he sees his theory as the most important thing ever he uses the phrase # # and # at seemingly every opportunity during the series we get it after about the first 10 minutes br br is diamond a little simplistic in the series sure the part about the # in south america is particularly amusing # some very long complicated history down to # swords and # # up the whole conquest of south america in about 15 minutes but the point remains valid these things did in fact contribute but not totally define the reasons for the # success against the established cultures br br is he preaching against western civilization in any way nope not a word not to my ear all he says is that luck played a large part in # which cultures advanced more quickly not that luck is the only reason br br in the end if you're looking for something that # your own sense of # then this series is not for you but if you are interested in all of the factors\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjbDflsuvX9L"
      },
      "source": [
        "# Architecture Diagram for LSTM Based Classifcation but you will have to change the\n",
        "# configuration/model parameters while implementing it depending on the input , output and the \n",
        "# Problem statement.\n",
        "\n",
        "from IPython.display import Image\n",
        "\n",
        "#Image(filename='LSTM_model.png')\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Opgqid3nvX9L",
        "outputId": "4816a38a-b512-4526-f1be-54be458a2c01"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "embedding_vector_length = 64\n",
        "model_lstm = tf.keras.Sequential()\n",
        "\n",
        "# Write the code for LSTM Based Classifcation\n",
        "# Embedding layer\n",
        "# LSTM Layer : You are free to choose the hyperparameters and the number of layers\n",
        "# Dense Layer\n",
        "# Use appropriate activation function in respective layers\n",
        "\n",
        "# Students will be starting their code from here:\n",
        "\n",
        "model_lstm.add(Embedding(10000, embedding_vector_length, input_length = average_review_length))\n",
        "model_lstm.add(LSTM(64))\n",
        "model_lstm.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    'models/LSTM.h5',\n",
        "    monitor='accuracy',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Students will be ending their code here\n",
        "\n",
        "model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model_lstm.summary())\n",
        "\n",
        "# Change the number of epochs and the batch size depending on the RAM Size\n",
        "\n",
        "#model_lstm.fit(X_train, y_train, epochs=5, batch_size=64, verbose=1, validation_data=(X_cv, y_cv))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 240, 64)           6400      \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 64)                33024     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 39,489\n",
            "Trainable params: 39,489\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXcin8axvX9M"
      },
      "source": [
        "# Final evaluation of the model using test dataset\n",
        "# Students will be starting their code from here:\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7qxt7eqvX9M"
      },
      "source": [
        "# High Level Model Architecture\n",
        "from IPython.display import Image\n",
        "\n",
        "#Image(filename='1_VGtBedNuZyX9E-07gnm2Yg.png')\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "jFQknMUtvX9M",
        "outputId": "e6d3a4c0-2b6c-42de-b7a0-9053ad70ca17"
      },
      "source": [
        "# create the model\n",
        "embedding_vector_length = 64\n",
        "model_cnn_lstm = Sequential()\n",
        "\n",
        "# Students will be starting their code from here:\n",
        "\n",
        "# Write the code for LSTM Based Classifcation\n",
        "# Embedding layer\n",
        "# Convolution-1D Layer : You are free to choose the hyperparameters and the number of layers\n",
        "# LSTM Layer : You are free to choose the hyperparameters and the number of layers\n",
        "# Dense Layer\n",
        "# Use appropriate activation function in respective layers\n",
        "\n",
        "\n",
        "# Students will be ending their code here\n",
        "\n",
        "model_cnn_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model_cnn_lstm.summary())\n",
        "\n",
        "# Change the number of epochs and the batch size depending on the RAM Size\n",
        "\n",
        "model_cnn_lstm.fit(X_train, y_train, epochs=5, batch_size=64, verbose=1, validation_data=(X_cv, y_cv))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-2513418f07b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmodel_cnn_lstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_cnn_lstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Change the number of epochs and the batch size depending on the RAM Size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[1;32m   2519\u001b[0m     \"\"\"\n\u001b[1;32m   2520\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2521\u001b[0;31m       raise ValueError('This model has not yet been built. '\n\u001b[0m\u001b[1;32m   2522\u001b[0m                        \u001b[0;34m'Build the model first by calling `build()` or calling '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2523\u001b[0m                        \u001b[0;34m'`fit()` with some data, or specify '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZuYwSFXvX9N"
      },
      "source": [
        "# Final evaluation of the CNN + RNN model using the test data\n",
        "# Students will be starting their code from here:\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohn9hXXmvX9N"
      },
      "source": [
        "# Identify and decode 5 of the sentences misclassified by LSTM model which  were correctly classified by CNN + LSTM model\n",
        "# Students will be starting their code from here:\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wXlRw4TvX9N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFb4l4nrvX9N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}